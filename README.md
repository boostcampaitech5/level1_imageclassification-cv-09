# Image Classication Competition ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦
![image](https://user-images.githubusercontent.com/72616557/228166051-e8197cb8-0025-485d-becc-cba4a5c257fd.png)

## About
ëª©í‘œ : ì‚¬ëŒì˜ ì •ë©´ ì‚¬ì§„ìœ¼ë¡œ 1. ë§ˆìŠ¤í¬ ì°©ìš© ì—¬ë¶€ 2. ì„±ë³„ 3. ë‚˜ì´ë¥¼ ì•„ë˜ì™€ ê°™ì´ 18ê°œ classë¡œ ì˜ˆì¸¡ 

<img src="https://user-images.githubusercontent.com/33598545/234359070-291d7e20-08c9-4824-ab56-70f4c16acd40.png" width="450" height="400" />

 
<mark>#ViT32</mark> <mark> #ViT16</mark> <mark> #ModelSoups</mark> <mark> #Relabeling</mark> <mark>#Oversampling</mark> <mark>#ContrastiveLearnig</mark> <mark> #WeightedAverageEnsemble</mark> <mark>   #HardVoting</mark> <mark> #SoftVoting</mark> <mark> #Optuna</mark> <mark> #Wandb</mark> 

****





  
## Setting Step
### 1. ê°€ìƒ í™˜ê²½ ì„¤ì¹˜  
```bash
conda env create -f environment.yml
conda activate model_soups
```
### 2. ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
- wandb, albumentations ë“± ì¶”ê°€ ì„¤ì¹˜  
### 3. pretrained model ë‹¤ìš´ë¡œë“œ  
- [Model soups](https://github.com/mlfoundations/model-soups/releases/tag/v0.0.2)ì—ì„œ ì œê³µí•œ ViT-B/32 ëª¨ë¸ ë‹¤ìš´
- ì´ 72ê°œ, ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ìµœëŒ€ 40ê°œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
```bash
python main.py --download-models --model-location <where models will be stored>  
```
****
## Training Step  
### 1. ViT-B/32, ViT-B/16
#### 1-1. Fine Tuning
```bash
python finetune.py --name {ëª¨ë¸ëª…} --i {ëª¨ë¸ number} --random-seed {ì‹œë“œ ì„¤ì •}
```
- [Model soups](https://github.com/mlfoundations/model-soups/releases/tag/v0.0.2)ì—ì„œ ì œê³µí•œ pretrained ëª¨ë¸ì„ 18ê°œì˜ class vectorë¥¼ outputìœ¼ë¡œ í•˜ëŠ” 1ê°œì˜ linear layerë¥¼ ì¶”ê°€í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤. 
- ViT-B/16 ì˜ ê²½ìš° Model soups pretrained weight ê°€ ì—†ìœ¼ë¯€ë¡œ clip ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ImageNet pretrained weight ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `--model {ViT-B/32 | ViT-B/16}` : base ëª¨ë¸ ì„¤ì •
- `--name` : ì €ì¥í•  ëª¨ë¸ ì´ë¦„
- `--i` : pretrained modelì˜ index
- `--random-seed` : random seed
- `--lr`, `--batch-size`, `--epochs`, `--data-location`, `--model-location` : learning rate, batch size, epoch, ë°ì´í„° ê²½ë¡œ, ì €ì¥í•  ëª¨ë¸ ê²½ë¡œ
<!-- - ì¶”ê°€ë¡œ learning rate, data-locationê³¼ ê°™ì€ argumentë“¤ì´ ìˆìœ¼ë©°, ëª¨ë“  argumentëŠ” default ê°’ì„ finetune.pyì—ì„œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. -->
- Tip : ì‰˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ìë™í™”í•˜ê¸°. training.sh íŒŒì¼ ì‘ì„± í›„ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰
<!-- - ì €ì¥ë˜ëŠ” ëª¨ë¸ pt íŒŒì¼ëª…ì€ "ëª¨ë¸ëª…i_epochs10.pt" -->
```bash
bash training.sh
``` 
#### 1-2. Data oversampling
<!-- ```bash  
python finetune.py --old-aug True
```   -->
- Age ì†ì„±ì˜ Old classì˜ ì ì€ train datasetìœ¼ë¡œ ì €í•˜ëœ í•™ìŠµ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ Old class dataë§Œì„ ì¶”ê°€ë¡œ Over samplingí•˜ì˜€ìŠµë‹ˆë‹¤.
- `--old-aug True` : Old class 1íšŒ ì¶”ê°€ over sampling
<!-- - í•´ë‹¹ dataì— augmentationì„ ë”°ë¡œ ì„¤ì •í•´ì£¼ê¸° ìœ„í•´, maskbasedataset.py ì—ì„œ get_transform í•¨ìˆ˜ì— ì¶”ê°€ë¡œ 'train2' augmentationì„ ì¶”ê°€í•´ ì£¼ì—ˆìŠµë‹ˆë‹¤. -->
#### 1-3. Loss Function ì„¤ì •
<!-- ```bash
python finetune.py --loss-fn {CrossEntropyLoss | ContrastiveLoss}
```   -->
- Interclassì˜ ê±°ë¦¬ë¥¼ ë„“íˆê³ , Intraclassì˜ ê±°ë¦¬ë¥¼ ì¢íˆëŠ” Contrastive Learningì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
- `--loss-fn` : ContrastiveLoss or CrossEntropyLoss, defaultëŠ” CrossEntropyLoss


### 2. Model Soups
- [Model soups](https://github.com/mlfoundations/model-soups/releases/tag/v0.0.2)ëŠ” ì—¬ëŸ¬ ê°œì˜ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ pretrained í•™ìŠµ ëª¨ë¸ë“¤ì„ ì¡°í•©í•˜ì—¬ í•˜ë‚˜ì˜ í•™ìŠµ ëª¨ë¸ì„ ë§Œë“œëŠ” ì•™ìƒë¸” ê¸°ë²•ì…ë‹ˆë‹¤. 
- ìˆ˜í–‰ ê³¼ì •ì„ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
1. ì—¬ëŸ¬ê°œì˜ pretrained modelì„ Test í•˜ì—¬ Accuracyë¥¼ ì–»ëŠ”ë‹¤.
2. Accuracy ê°’ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œë‹¤.
3. ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ê³¼ì˜ weightê°’ì„ averageí•˜ì—¬ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ìƒì„±í•œë‹¤.
4. ìƒì„±ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •ì„ í•˜ì˜€ì„ ë•Œ í˜„ì¬ê¹Œì§€ ê°€ì¥ ì¢‹ì€ Accuracyë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìœ¼ë©´ ì €ì¥í•˜ê³ , 3, 4ë²ˆì„ ë°˜ë³µí•œë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ averageí•˜ì§€ ì•Šê³  3, 4ë²ˆì„ ë°˜ë³µí•œë‹¤.
5. ê°€ì¥ Accuracyê°€ ë†’ì€ ëª¨ë¸ì„ ìµœì¢… ëª¨ë¸ë¡œ ì„ ì •í•œë‹¤. 
#### 2-1. Fine Tuning
<!-- ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì—, 1-1 ì˜ Fine Tuningì—ì„œ "--model ViT-B/32" argumentë¥¼ ì´ìš©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. -->
- [Model soups](https://github.com/mlfoundations/model-soups/releases/tag/v0.0.2)ì—ì„œ ì œê³µí•œ pretrained modelì€ ViT-B/32 ëª¨ë¸ì…ë‹ˆë‹¤.
- 1ë²ˆê³¼ ë™ì¼í•˜ê²Œ Fine tuningì„ ì§„í–‰í•©ë‹ˆë‹¤.
<!-- - ì´ ì™¸ì˜ ì‚¬ìš©ë²•ì€ 1-1,2,3ê³¼ ë™ì¼í•©ë‹ˆë‹¤. -->

#### 2-2. Individual Evaluation  
```bash
python main.py --eval-individual-models --name {ëª¨ë¸ëª…} --model-num {ëª¨ë¸ ê°œìˆ˜} --random-seed {ëœë¤ ì‹œë“œ}
```
- finetuneì„ í†µí•´ ë§Œë“  ëª¨ë¸ë“¤ì˜ accuracyë¥¼ ì¸¡ì •í•˜ì—¬ ê¸°ë¡í•©ë‹ˆë‹¤. 
- `--name` : ì €ì¥ëœ ëª¨ë¸ëª…
- `--model-num` : Evaludationí•  ëª¨ë¸ì˜ ê°œìˆ˜
- `--random-seed` : ëœë¤ ì‹œë“œ 
- `--val-ratio`, `--epoch`, `--data-location`, `--model-locatoin` : validation dataset ë¹„ìœ¨, epoch, ë°ì´í„°ì…‹ ê²½ë¡œ, ì €ì¥í•  ëª¨ë¸ ê²½ë¡œ
<!-- - val_ratioì— None ê°’ì„ ì…ë ¥í•˜ë©´, ì „ì²´ datasetì— ëŒ€í•´ evaludationì„ ì§„í–‰í•©ë‹ˆë‹¤. 
- finetune ë‹¹ì‹œì— random-seedë¥¼ ì„¤ì •í•´ì£¼ì—ˆë‹¤ë©´, Noneê°’ì„ ë„£ì–´ì£¼ë©´ ì•ˆë©ë‹ˆë‹¤. -->
- ì‹¤í–‰ì´ ì™„ë£Œë˜ë©´ logs í´ë” ì•ˆì— ê° ëª¨ë¸ì˜ accuracyê°€ ì íŒ jsonl íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤. 

#### 2-3. Greedy Soup
```bash
python main.py --greedy-soup --name {ëª¨ë¸ëª…} --model-num {ëª¨ë¸ ê°œìˆ˜} --random-seed {ëœë¤ ì‹œë“œ}
```  
- individual Evaluationì—ì„œ ì €ì¥í•œ ì—¬ëŸ¬ ëª¨ë¸ì˜ accuracyì •ë³´ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤. ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë¶ˆëŸ¬ì™€ greedyí•˜ê²Œ ì¡°í•©í•˜ì—¬(averaging) ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ë„ë¡ í•˜ëŠ” ìµœì¢… ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
- `--name` : ì €ì¥ëœ ëª¨ë¸ëª…
- `--model-num` : Evaludationí•  ëª¨ë¸ì˜ ê°œìˆ˜
- `--random-seed` : ëœë¤ ì‹œë“œ 
- `--val-ratio`, `--epoch`, `--data-location`, `--model-locatoin` : validation dataset ë¹„ìœ¨, epoch, ë°ì´í„°ì…‹ ê²½ë¡œ, ì €ì¥í•  ëª¨ë¸ ê²½ë¡œ
- ì‹¤í–‰ ê²°ê³¼ model í´ë” ì•ˆì— ìµœì¢… ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤.
- log í´ë” ì•ˆì— ë³€ìˆ˜ GREEDY_SOUP_LOG_FILEê°€ ì´ë¦„ì„ ë¡œê·¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. í•´ë‹¹ ë¡œê·¸ì—ëŠ” averagingëœ ëª¨ë¸ ì •ë³´ê°€ ì €ì¥ë©ë‹ˆë‹¤.

****
## Inference Step
### 1. ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„ w/ Validation dataset
```bash
python validation.py --model-name {ëª¨ë¸ëª….pt íŒŒì¼}
```
- Validation setì—ì„œ classë³„ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë¹„ìœ¨ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
- í•´ë‹¹ ëª¨ë¸ì„ í•™ìŠµí–ˆì„ ë•Œ, ì‚¬ìš©í–ˆë˜ random seed ê°’ì„ ë™ì¼í•˜ê²Œ ìœ ì§€í•´ ì£¼ì–´ì•¼ ì •í™•í•œ í™•ë¥ ê³¼ ì˜ˆì¸¡ê°’ì´ ë‚˜ì˜µë‹ˆë‹¤.
- `--model-name` : evaluationí•  ëª¨ë¸ëª…, 
- `--i` : pretrained modelì˜ index
- `--random-seed` : ëœë¤ ì‹œë“œ


#### 1-1. Weighted Average Ensemble  
- Age classì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë†’ì´ê³ ì Age ì†ì„±ë§Œì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬, ì´ë¥¼ ì „ì²´ class(18ê°œ) ë¶„ë¥˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ weighted sumì„ í•˜ì˜€ìŠµë‹ˆë‹¤.
- `--weighted-ensemble` : Age classë¥¼ í•™ìŠµí•œ ëª¨ë¸ëª…, DefaultëŠ” None
```
python finetune_age.py --name {ëª¨ë¸ëª…} --i {ëª¨ë¸ number} --random-seed {ì‹œë“œ ì„¤ì •}
```
- finetune_age.pyëŠ” Age classë§Œì„ í•™ìŠµí•©ë‹ˆë‹¤.
- `--name`, `--i`, `--random-seed`ëŠ” finetune.pyì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
#### 1-2. Soft voting (Ensemble)  
- 2ê°œì˜ í•™ìŠµ ëª¨ë¸ì˜ ê° classì˜ í™•ë¥ ê°’ì„ minmax scaling í›„ ë”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. 
- `--soft-voting` : soft votingí•  ëª¨ë¸ëª…, DefaultëŠ” None 
#### 1-3. Hard voting (Ensemble)
- ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ csv íŒŒì¼ì˜ ì—¬ëŸ¬ê°œë¥¼ ìµœì¢…ì ìœ¼ë¡œ Hard votingì„ ìˆ˜í–‰í•˜ì—¬ Ensembleì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.
- hard_voting.ipynb ì„ ì‹¤í–‰í•˜ì—¬, ì•™ìƒë¸”ì„ ì›í•˜ëŠ” csvë¥¼ ê°€ì§€ê³  hard votingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ì•„ë˜ ê·¸ë¦¼ì€ ì¶œë ¥ ì˜ˆì‹œì…ë‹ˆë‹¤.   
<!-- ![image](https://user-images.githubusercontent.com/113486402/234260857-a5175967-8a7c-4c0b-bcfd-a63f7fb1559c.png) -->
<img src="https://user-images.githubusercontent.com/113486402/234260857-a5175967-8a7c-4c0b-bcfd-a63f7fb1559c.png" width="300" height="500" />


### 2. Test w/ Test dataset
```bash
python inference.py --model-name {ëª¨ë¸ëª….pt íŒŒì¼}
```
- ìƒì„±í•œ ëª¨ë¸ íŒŒì¼(.pt)ë¥¼ ì´ìš©í•˜ì—¬ Test dataë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.  
- `--model-name` : inferenceí•  ëª¨ë¸ëª…
- `--weighted-ensemble`, `--soft-voting` : Weighted average ensemble ì‹œ ëª¨ë¸ëª…, Soft Voting ì‹œ ëª¨ë¸ëª… 
<!-- - argument ë‚´ì— pt íŒŒì¼ëª…ì„ ì ê³  ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤.    -->
- ìµœì¢… ê²°ê³¼ csv íŒŒì¼ì´ output í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤. 


****
## Additional Step
### 1. Dataset Relabeling  
![image](https://user-images.githubusercontent.com/113486402/233954582-70a43065-7586-483e-abf5-707e744eebb3.png)  
<!-- - relabelingì´ í•„ìš”í•œ id ëª©ë¡ì„ listì— ë„£ì–´ì„œ relabel_dict ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. -->
- ì˜ëª» ë¼ë²¨ë§ëœ ë°ì´í„° id ëª©ë¡ì„ ë‹´ì€ relabel_dict ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Relabelingì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.
<!-- - maskbasedataset.py íŒŒì¼ì—ì„œ ì¶”ê°€ë¡œ relabelingì´ í•„ìš”í•œ idê°€ ìˆë‹¤ë©´ ê°„ë‹¨í•˜ê²Œ í•´ë‹¹ listì— ë„£ì–´ì£¼ê¸°ë§Œ í•˜ë©´ relabelingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.   -->

### 2. Hyperparameter Tuning  
```bash
python optuna_script.py
```
- Optunaë¥¼ ì´ìš©í•˜ì—¬ Hyper paramter tuningì„ ì§„í–‰í•©ë‹ˆë‹¤.
- optuna_script.py íŒŒì¼ì—ì„œ hyper parameter tuningì„ ìœ„í•œ ì„¤ì •ì„ ì•„ë˜ ì‚¬ì§„ê³¼ ê°™ì´ ë„£ì–´ì£¼ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.  

****
## Result
- Private score 3rd / F1 score - 0.7613 / Accuracy - 81.3175
- Public score 6th / F1 score - 0.7653 / Accuracy - 81.3968
![í™”ë©´ ìº¡ì²˜ 2023-04-26 022440](https://user-images.githubusercontent.com/33598545/234355466-63a4c6c0-1b86-4039-a327-15bcf7758db1.png)


****


## Contributors

|ì‹ í˜„ì¤€ |                                                  í•œí˜„ë¯¼|ì •í˜„ì„ |                                                  ê¹€ì§€ë²”|ì˜¤ìœ ë¦¼|
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| [<img src="https://avatars.githubusercontent.com/u/113486402?s=400&v=4" alt="" style="width:100px;100px;">](https://github.com/june95) <br/> | [<img src="https://avatars.githubusercontent.com/u/33598545?s=400&u=d0aaa9e96fd2fa1d0c1aa034d8e9e2c8daf96473&v=4" alt="" style="width:100px;100px;">](https://github.com/Hyunmin-H) <br/> | [<img src="https://avatars.githubusercontent.com/u/72616557?v=4" alt="" style="width:100px;100px;">](https://github.com/hyuns66) <br/> | [<img src="https://avatars.githubusercontent.com/u/91449518?v=4" alt="" style="width:100px;100px;">](https://github.com/jibeomkim7) <br/> |[<img src="https://avatars.githubusercontent.com/u/63313306?s=400&u=094cba544d8029b4f93aa191d036a109d6265fa8&v=4" alt="" style="width:100px;100px;">](https://github.com/jennifer060697) <br/> |

****
## Reference


Model soups : [Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482).

ViT : https://github.com/google-research/vision_transformer

ContrastiveLoss : https://github.com/KevinMusgrave/pytorch-metric-learning