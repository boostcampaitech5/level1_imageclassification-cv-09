# ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦

ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AItech 5ê¸° CV_9íŒ€ level-1(image classification) í”„ë¡œì íŠ¸ ê³µê°„ì…ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/72616557/228166051-e8197cb8-0025-485d-becc-cba4a5c257fd.png)



## Contributors

|ì‹ í˜„ì¤€ |                                                  í•œí˜„ë¯¼|ì •í˜„ì„ |                                                  ê¹€ì§€ë²”|ì˜¤ìœ ë¦¼|
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| [<img src="https://avatars.githubusercontent.com/u/113486402?s=400&v=4" alt="" style="width:100px;100px;">](https://github.com/june95) <br/> | [<img src="https://avatars.githubusercontent.com/u/33598545?s=400&u=d0aaa9e96fd2fa1d0c1aa034d8e9e2c8daf96473&v=4" alt="" style="width:100px;100px;">](https://github.com/Hyunmin-H) <br/> | [<img src="https://avatars.githubusercontent.com/u/72616557?v=4" alt="" style="width:100px;100px;">](https://github.com/hyuns66) <br/> | [<img src="https://avatars.githubusercontent.com/u/91449518?v=4" alt="" style="width:100px;100px;">](https://github.com/jibeomkim7) <br/> |[<img src="https://avatars.githubusercontent.com/u/63313306?s=400&u=094cba544d8029b4f93aa191d036a109d6265fa8&v=4" alt="" style="width:100px;100px;">](https://github.com/jennifer060697) <br/> |


í•´ë‹¹ í”„ë¡œì íŠ¸ repositoryì—ì„œ ì°¸ê³ í•œ reference ëª©ë¡ì…ë‹ˆë‹¤.

Model soups : [Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482).

ViT : https://github.com/google-research/vision_transformer

  
## Setting Step
### 1. ê°€ìƒ í™˜ê²½ ì„¤ì¹˜  
```bash
conda env create -f environment.yml
conda activate model_soups
```
### 2. ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
- wandb, albumentations ë“± ì¶”ê°€ ì„¤ì¹˜  
### 3. pretrained model ë‹¤ìš´ë¡œë“œ  
```bash
python main.py --download-models --model-location <where models will be stored>  
```
  
## ì‹¤í–‰ Step  
### 1. Fine Tuning
```bash
python finetune.py --name {ëª¨ë¸ëª…} --i {ëª¨ë¸ number} --batch-size {ë°°ì¹˜ ì‚¬ì´ì¦ˆ(ex:256)} --epochs {ì—í­ ìˆ˜(ex:10)} --random-seed {ì‹œë“œ ì„¤ì •}
```
- ImageNet ë“±ì„ ì´ìš©í•˜ì—¬ ë¯¸ë¦¬ í•™ìŠµí•œ ëª¨ë¸ parameterë¥¼ ì´ìš©í•˜ì—¬, ìš°ë¦¬ì˜ ë°ì´í„°ì…‹ì— ë§ê²Œ ë§ˆì§€ë§‰ layerë¥¼ ë°”ê¿”ì£¼ê³  í•™ìŠµí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
- ëª¨ë¸ number rangeëŠ” 0~71(72ê°œ ì…ë‹ˆë‹¤.)  
- ì €ì¥ë˜ëŠ” ëª¨ë¸ pt íŒŒì¼ëª…ì€ "ëª¨ë¸ëª…i_epochs10.pt"
- ì¶”ê°€ë¡œ learning rate, data-locationê³¼ ê°™ì€ argumentë“¤ì´ ìˆìœ¼ë©°, ëª¨ë“  argumentëŠ” default ê°’ì„ finetune.pyì—ì„œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Tip : ì‰˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ìë™í™”í•˜ê¸° -> training.sh íŒŒì¼ ì‘ì„± í›„ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰
```bash
bash trining.sh
```  

#### 1-1. Data oversampling ì—¬ë¶€ ì„¤ì •
```bash  
python finetune.py --old-aug True
```  
- ì €í¬ëŠ” Old classì˜ train datasetì´ ì ì€ ê²ƒì„ ì–´ëŠì •ë„ í•´ê²°í•˜ê¸° ìœ„í•´ Old class dataë§Œ ì¶”ê°€ë¡œ over sampling í•˜ëŠ” ì½”ë“œ ë˜í•œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.  
- finetune íŒŒì¼ì„ ì‹¤í–‰í•  ë•Œì—, <--old-aug True> ë¡œ argumentë¥¼ ì¶”ê°€í•´ì£¼ë©´, Old class dataë§Œ í•œ ë²ˆ ë” ì¶”ê°€í•˜ì—¬ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.
- í•´ë‹¹ dataì— augmentationì„ ë”°ë¡œ ì„¤ì •í•´ì£¼ê¸° ìœ„í•´, maskbasedataset.py ì—ì„œ get_transform í•¨ìˆ˜ì— ì¶”ê°€ë¡œ 'train2' augmentationì„ ì¶”ê°€í•´ ì£¼ì—ˆìŠµë‹ˆë‹¤.

#### 1-2. Loss Function ì„¤ì •
```bash
python finetune.py --loss-fn {CrossEntropyLoss | ContrastiveLoss}
```  
- 


### 2. Individual Evaluation  
```bash
python main.py --eval-individual-models --name {ëª¨ë¸ëª…}
```
- finetuneì„ í†µí•´ ë§Œë“  ëª¨ë¸ë“¤ì˜ accuracyë¥¼ ì¸¡ì •í•˜ì—¬ ê¸°ë¡í•´ë‘ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
![image](https://user-images.githubusercontent.com/113486402/233948441-7bab18bc-37f8-424b-a0fb-3223a37781b8.png)
- "ì…ë ¥í•˜ì„¸ìš”" ë¶€ë¶„ì— ì¸¡ì •í•  ëª¨ë¸ì˜ ê°œìˆ˜(NUM_MODELS), ì‚¬ìš©í•  epoch, val_ratioë¥¼ ì ì–´ì¤ë‹ˆë‹¤.
- val_ratioì— None ê°’ì„ ì…ë ¥í•˜ë©´, ì „ì²´ datasetì— ëŒ€í•´ evaludationì„ ì§„í–‰í•©ë‹ˆë‹¤. 
- finetune ë‹¹ì‹œì— random-seedë¥¼ ì„¤ì •í•´ì£¼ì—ˆë‹¤ë©´, Noneê°’ì„ ë„£ì–´ì£¼ë©´ ì•ˆë©ë‹ˆë‹¤.
- ì‹¤í–‰ ê²°ê³¼ë¡œ logs í´ë” ì•ˆì— ê° ëª¨ë¸ì˜ accuracyê°€ ì íŒ jsonl íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤. 

### 3. Greedy Soup
```bash
python main.py --greedy-soup --name {ëª¨ë¸ëª…}
```  
- individual Evaluationì—ì„œ ì €ì¥í•œ ì—¬ëŸ¬ ëª¨ë¸ì˜ accuracyì •ë³´ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.  
- ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë¶ˆëŸ¬ì™€ greedyí•˜ê²Œ ì¡°í•©í•˜ì—¬(averaging) ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ë„ë¡ í•˜ëŠ” ìµœì¢… ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
- ì‹¤í–‰ ê²°ê³¼ model í´ë” ì•ˆì— greedy ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤.
- log í´ë” ì•ˆì— ë³€ìˆ˜ GREEDY_SOUP_LOG_FILEê°€ ì´ë¦„ì„ ë¡œê·¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. í•´ë‹¹ ë¡œê·¸ì—ëŠ” averagingëœ ëª¨ë¸ ì •ë³´ê°€ ì €ì¥ë©ë‹ˆë‹¤.

### 4. Inference
```bash
python inference.py
```
- ìƒì„±í•œ ëª¨ë¸ íŒŒì¼(.pt)ë¥¼ ì´ìš©í•˜ì—¬ Test dataë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.  
- "ì…ë ¥í•˜ì„¸ìš”" ì£¼ì„ ë‚´ì— pt íŒŒì¼ëª…ì„ ì ê³  ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤.   
![image](https://user-images.githubusercontent.com/113486402/233952932-ea2967b4-a934-4238-a08f-7b1e85f6031d.png)
- ìµœì¢… ì˜ˆì¸¡í•œ csv íŒŒì¼ì´ output í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.  

#### 4-1. Hard voting (Ensemble)
- inference.py ë¥¼ í†µí•´ ì˜ˆì¸¡ëœ output.csv ì—¬ëŸ¬ê°œì˜ ê²°ê³¼ê°’ì„ ê°€ì§€ê³  ìµœì¢…ì ìœ¼ë¡œ hard votingì„ ìˆ˜í–‰í•˜ëŠ” Ensemble ë˜í•œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.  
- hard_voting.ipynb ì„ ì‹¤í–‰í•˜ì—¬, ì•™ìƒë¸”ì„ ì›í•˜ëŠ” csvë¥¼ ê°€ì§€ê³  hard votingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
  
## ì¶”ê°€ ê¸°ëŠ¥
### 1. Relabeling  
![image](https://user-images.githubusercontent.com/113486402/233954582-70a43065-7586-483e-abf5-707e744eebb3.png)  
- relabelingì´ í•„ìš”í•œ id ëª©ë¡ì„ listì— ë„£ì–´ì„œ relabel_dict ë”•ì…”ë„ˆë¦¬ì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤.
- maskbasedataset.pyì—ì„œ ì¶”ê°€ë¡œ relabelingì´ í•„ìš”í•œ idê°€ ìˆë‹¤ë©´ ê°„ë‹¨í•˜ê²Œ í•´ë‹¹ listì— ë„£ì–´ì£¼ê¸°ë§Œ í•˜ë©´ relabelingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  

### 2. Optuna  
- 




### Note

If you want, you can all steps with:
```bash
python main.py --download-models --eval-individual-models --uniform-soup --greedy-soup --plot --data-location <where data is stored> --model-location <where models are stored>
```

Also note: if you are interested in running ensemble baselines, check out [the ensemble branch](https://github.com/mlfoundations/model-soups/tree/ensemble).

Also note: if you are interested in running a minial example of [wise-ft](https://arxiv.org/abs/2109.01903), you can run `python wise-ft-example.py --download-models`. 

Also note: if you are interested in running minimal examples of zeroshot/fine-tuning, you can run `python zeroshot.py` or `python finetune.py`. See program arguments (i.e., run with `--help`) for more information. Note that these are minimal examples and do not contain rand-aug, mixup, or LP-FT.

### Questions

If you have any questions please feel free to raise an issue. If there are any FAQ we will answer them here.

### Authors

This project is by the following authors, where * denotes equal contribution (alphabetical ordering):
- [Mitchell Wortsman](https://mitchellnw.github.io/)
- [Gabriel Ilharco](http://gabrielilharco.com/)
- [Samir Yitzhak Gadre](https://sagadre.github.io/)
- [Rebecca Roelofs](https://twitter.com/beccaroelofs)
- [Raphael Gontijo-Lopes](https://raphagl.com/)
- [Ari S. Morcos](http://www.arimorcos.com/)
- [Hongseok Namkoong](https://hsnamkoong.github.io/)
- [Ali Farhadi](https://homes.cs.washington.edu/~ali/)
- [Yair Carmon*](https://www.cs.tau.ac.il/~ycarmon/)
- [Simon Kornblith*](https://simonster.com/)
- [Ludwig Schmidt*](https://people.csail.mit.edu/ludwigs/)


## Citing

If you found this repository useful, please consider citing:
```bibtex
@InProceedings{pmlr-v162-wortsman22a,
  title = 	 {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author =       {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {23965--23998},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wortsman22a.html}
}


```
